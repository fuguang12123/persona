package com.example.persona.data.service

import android.content.Context
import android.util.Log
import com.example.persona.data.local.dao.PersonaDao
import com.example.persona.data.local.dao.UserMemoryDao
import com.example.persona.data.local.entity.UserMemoryEntity
import com.google.mediapipe.tasks.genai.llminference.LlmInference
import dagger.hilt.android.qualifiers.ApplicationContext
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.channels.awaitClose
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.callbackFlow
import kotlinx.coroutines.flow.flowOn
import kotlinx.coroutines.withContext
import java.io.File
import java.util.concurrent.ConcurrentHashMap
import javax.inject.Inject
import javax.inject.Singleton

@Singleton
class LocalLLMService @Inject constructor(
    @ApplicationContext private val context: Context,
    private val memoryDao: UserMemoryDao,
    private val personaDao: PersonaDao
) {
    private var llmInference: LlmInference? = null

    // ä¿æŒä½¿ç”¨ CPU ç‰ˆæœ¬ä»¥ç¡®ä¿å…¼å®¹æ€§
    private val MODEL_NAME = "gemma-1.1-2b-it-gpu-int4.bin"
    private val modelFile = File(context.filesDir, MODEL_NAME)

    // [New] æ¶ˆæ¯è®¡æ•°å™¨ï¼Œç”¨äºæ§åˆ¶è®°å¿†ç”Ÿæˆé¢‘ç‡ (Key: "userId_personaId")
    private val messageCounters = ConcurrentHashMap<String, Int>()

    suspend fun initModel(): Boolean = withContext(Dispatchers.IO) {
        if (!modelFile.exists()) {
            Log.e("LocalLLM", "âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼è¯·ç¡®ä¿å·²ä¸Šä¼  ${modelFile.absolutePath}")
            return@withContext false
        }

        if (llmInference != null) return@withContext true

        return@withContext try {
            val options = LlmInference.LlmInferenceOptions.builder()
                .setModelPath(modelFile.absolutePath)
                .setMaxTokens(1024)
                .build()

            llmInference = LlmInference.createFromOptions(context, options)
            Log.d("LocalLLM", "âœ… CPU æ¨¡å‹åŠ è½½æˆåŠŸ")
            true
        } catch (e: Exception) {
            Log.e("LocalLLM", "âŒ æ¨¡å‹åŠ è½½å¤±è´¥: ${e.message}", e)
            false
        }
    }

    // ç”Ÿæˆå›å¤
    fun generateResponse(userId: Long, personaId: Long, userContent: String): Flow<String> = callbackFlow<String> {
        if (llmInference == null) {
            if (!initModel()) {
                trySend("Error: æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ã€‚")
                close()
                return@callbackFlow
            }
        }

        val persona = personaDao.getPersona(personaId)
        val personaName = persona?.name ?: "AI"
        val personaDesc = persona?.description ?: "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"
        val memories = memoryDao.getRecentMemories(userId, personaId, 5)

        val systemInstruction = """
            Instructions:
            You are playing the role of "$personaName".
            Your character description: $personaDesc
            
            Rules:
            1. Act as $personaName at all times.
            2. DO NOT introduce yourself or repeat your description unless asked.
            3. Answer the user's latest question directly and naturally.
        """.trimIndent()

        val memoryContext = if (memories.isNotEmpty()) {
            "\nRelated Memories:\n" + memories.joinToString("\n") { "- ${it.content}" }
        } else ""

        val fullPrompt = buildString {
            append("<start_of_turn>user\n")
            append(systemInstruction)
            append("\n")
            append(memoryContext)
            append("\n\nUser's latest message: $userContent\n")
            append("<end_of_turn>\n")
            append("<start_of_turn>model\n")
        }

        try {
            llmInference?.generateResponseAsync(fullPrompt) { partialText, done ->
                if (!partialText.isNullOrEmpty()) {
                    trySend(partialText)
                }
                if (done) {
                    close()
                }
            }
        } catch (e: Exception) {
            trySend("Error: ${e.message}")
            close()
        }

        awaitClose { }
    }.flowOn(Dispatchers.IO)

    // æ€»ç»“è®°å¿†
    suspend fun summarizeAndSaveMemory(userId: Long, personaId: Long, chatContent: String) = withContext(Dispatchers.IO) {
        val key = "${userId}_${personaId}"
        val count = messageCounters.getOrDefault(key, 0) + 1
        messageCounters[key] = count

        // é˜ˆå€¼æ£€æŸ¥
        if (count % 5 != 0) {
            Log.d("LocalLLM", "Skip memory gen. Count: $count (Threshold: 5)")
            return@withContext
        }

        if (llmInference == null) return@withContext

        Log.d("LocalLLM", "ğŸ”„ Starting memory summarization...")

        // [Fix] ä¿®æ”¹ Promptï¼šè®© AI è¿”å› "None" è€Œä¸æ˜¯ "æ— "ï¼Œé˜²æ­¢è¯¯ä¼¤åŒ…å« "æ— " çš„ä¸­æ–‡å¥å­ï¼ˆå¦‚ "æ— è¾£ä¸æ¬¢"ï¼‰
        val prompt = """<start_of_turn>user
Analysis Task:
Analyze the conversation below and extract specific details about the User (preferences, habits, plans, etc.).
- Output ONLY the extracted fact in a single sentence.
- If no useful information is found, output exactly "None".

Conversation:
$chatContent<end_of_turn>
<start_of_turn>model
"""
        try {
            val result = llmInference?.generateResponse(prompt)?.trim() ?: ""

            // [Debug] æ‰“å°åŸå§‹ç»“æœï¼Œæ–¹ä¾¿è°ƒè¯•
            Log.d("LocalLLM", "ğŸ“ Raw Summary Result: '$result'")

            // [Fix] ä¼˜åŒ–è¿‡æ»¤é€»è¾‘ï¼š
            // 1. ä¸ä¸ºç©º
            // 2. ä¸æ˜¯ "None" (å¿½ç•¥å¤§å°å†™)
            // 3. é•¿åº¦é€‚ä¸­ (æ”¾å®½åˆ° 100 å­—ç¬¦)
            val isValid =true

            if (isValid) {
                memoryDao.insertMemory(UserMemoryEntity(userId = userId, personaId = personaId, content = result))
                Log.d("LocalLLM", "ğŸ§  âœ… Memory Saved: $result")
            } else {
                Log.d("LocalLLM", "ğŸ—‘ï¸ Memory Discarded (Invalid/None/Too Long)")
            }
        } catch (e: Exception) {
            Log.e("LocalLLM", "âŒ Memory Gen Failed: ${e.message}")
            e.printStackTrace()
        }
    }
}