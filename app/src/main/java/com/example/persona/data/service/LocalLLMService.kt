package com.example.persona.data.service

import android.content.Context
import android.util.Log
import com.example.persona.data.local.dao.PersonaDao
import com.example.persona.data.local.dao.UserMemoryDao
import com.example.persona.data.local.entity.UserMemoryEntity
import com.google.mediapipe.tasks.genai.llminference.LlmInference
import dagger.hilt.android.qualifiers.ApplicationContext
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.channels.awaitClose
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.callbackFlow
import kotlinx.coroutines.flow.flowOn
import kotlinx.coroutines.withContext
import java.io.File
import java.util.concurrent.ConcurrentHashMap
import javax.inject.Inject
import javax.inject.Singleton

@Singleton
class LocalLLMService @Inject constructor(
    @ApplicationContext private val context: Context,
    private val memoryDao: UserMemoryDao,
    private val personaDao: PersonaDao
) {
    /**
     * @class com.example.persona.data.service.LocalLLMService
     * @description ç«¯ä¾§å¤§è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡ï¼Œè´Ÿè´£åœ¨â€œç§å¯†æ¨¡å¼â€ä¸‹ä»¥æœ¬åœ°æ¨¡å‹å®Œæˆå¯¹è¯ç”Ÿæˆä¸è®°å¿†æ‘˜è¦ã€‚é€šè¿‡ `callbackFlow` å®ç°æµå¼è¾“å‡ºï¼Œç»“åˆ IO è°ƒåº¦ä¿è¯æ¨ç†ä¸æ•°æ®åº“å†™å…¥çš„çº¿ç¨‹éš”ç¦»ã€‚æœåŠ¡é‡‡ç”¨ Gemma é‡åŒ–æ¨¡å‹ï¼ˆç¤ºä¾‹ï¼‰ï¼Œå¹¶æä¾›æ¨¡å‹åˆå§‹åŒ–æ£€æŸ¥ã€åˆ†ç‰‡è¾“å‡ºã€é˜ˆå€¼è®°å¿†æ‘˜è¦ç­‰æœºåˆ¶ï¼Œæ»¡è¶³ã€Šæœ€ç»ˆä½œä¸š.mdã€‹è¿›é˜¶æŒ‘æˆ˜ä¸­çš„ç«¯äº‘ååŒæ··åˆæ¶æ„ï¼ˆC4ï¼‰ä¸å¯Œäº¤äº’ä½“éªŒï¼ˆC1ï¼‰ã€‚
     * @author Persona Team <persona@project.local>
     * @version v1.0.0
     * @since 2025-11-30
     * @see com.example.persona.data.repository.ChatRepository
     * @å…³è”åŠŸèƒ½ REQ-C1 å¯Œæ–‡æœ¬ä¸æµå¼è¾“å‡ºï¼›REQ-C4 ç«¯äº‘ååŒæ··åˆæ¶æ„
     */
    private var llmInference: LlmInference? = null

    // ä¿æŒä½¿ç”¨ CPU ç‰ˆæœ¬ä»¥ç¡®ä¿å…¼å®¹æ€§
    private val MODEL_NAME = "gemma-1.1-2b-it-gpu-int4.bin"
    private val modelFile = File(context.filesDir, MODEL_NAME)

    // [New] æ¶ˆæ¯è®¡æ•°å™¨ï¼Œç”¨äºæ§åˆ¶è®°å¿†ç”Ÿæˆé¢‘ç‡ (Key: "userId_personaId")
    private val messageCounters = ConcurrentHashMap<String, Int>()

    /**
     * åŠŸèƒ½: åˆå§‹åŒ–ç«¯ä¾§æ¨ç†æ¨¡å‹ï¼ˆIO çº¿ç¨‹ï¼‰ï¼Œæ ¡éªŒæ–‡ä»¶å­˜åœ¨å¹¶åˆ›å»ºæ¨ç†å™¨ã€‚
     * å®ç°é€»è¾‘: æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ -> æ„å»º Options -> åˆ›å»º `LlmInference`ã€‚
     * è¿”å›å€¼: Boolean - true è¡¨ç¤ºåˆå§‹åŒ–æˆåŠŸï¼›false è¡¨ç¤ºç¼ºå¤±æˆ–å¼‚å¸¸ã€‚
     * å…³è”åŠŸèƒ½: REQ-C4 ç«¯äº‘ååŒ-ç«¯ä¾§æ¨¡å‹é›†æˆ
     */
    suspend fun initModel(): Boolean = withContext(Dispatchers.IO) {
        if (!modelFile.exists()) {
            Log.e("LocalLLM", "âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼è¯·ç¡®ä¿å·²ä¸Šä¼  ${modelFile.absolutePath}")
            return@withContext false
        }

        if (llmInference != null) return@withContext true

        return@withContext try {
            val options = LlmInference.LlmInferenceOptions.builder()
                .setModelPath(modelFile.absolutePath)
                .setMaxTokens(1024)
                .build()

            llmInference = LlmInference.createFromOptions(context, options)
            Log.d("LocalLLM", "âœ… CPU æ¨¡å‹åŠ è½½æˆåŠŸ")
            true
        } catch (e: Exception) {
            Log.e("LocalLLM", "âŒ æ¨¡å‹åŠ è½½å¤±è´¥: ${e.message}", e)
            false
        }
    }

    // ç”Ÿæˆå›å¤
    /**
     * åŠŸèƒ½: åŸºäºç«¯ä¾§æ¨¡å‹ç”Ÿæˆæµå¼å›å¤ï¼Œè¿”å› `Flow<String>` çš„åˆ†ç‰‡æ–‡æœ¬ï¼Œä¾› UI æ¸è¿›å‘ˆç°ã€‚
     * å®ç°é€»è¾‘:
     * 1. è‹¥æœªåˆå§‹åŒ–åˆ™å°è¯•åˆå§‹åŒ–æ¨¡å‹
     * 2. æ‹¼æ¥è§’è‰²è®¾å®šã€è®°å¿†ä¸Šä¸‹æ–‡ä¸ç”¨æˆ·æ¶ˆæ¯ï¼Œæ„é€ å®Œæ•´ Prompt
     * 3. é€šè¿‡ `generateResponseAsync` æ¨é€åˆ†ç‰‡è‡³ Flowï¼Œç»“æŸæ—¶å…³é—­æµ
     * @param userId Long - ç”¨æˆ·ID
     * @param personaId Long - Persona ID
     * @param userContent String - ç”¨æˆ·è¾“å…¥
     * @return Flow<String> - æ–‡æœ¬åˆ†ç‰‡æµ
     * å…³è”åŠŸèƒ½: REQ-C1 æµå¼è¾“å‡ºï¼›REQ-C4 ç«¯äº‘ååŒ-ç§èŠç«¯ä¾§è·¯å¾„
     * å¤æ‚åº¦åˆ†æ: æ—¶é—´ O(T)ï¼ˆä¸ç”Ÿæˆé•¿åº¦ç›¸å…³ï¼‰| ç©ºé—´ O(1)
     * çº¿ç¨‹å®‰å…¨: æ˜¯ - `flowOn(Dispatchers.IO)` ç¡®ä¿åœ¨ IO çº¿ç¨‹æ¨ç†
     */
    fun generateResponse(userId: Long, personaId: Long, userContent: String): Flow<String> = callbackFlow<String> {
        if (llmInference == null) {
            if (!initModel()) {
                trySend("Error: æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ã€‚")
                close()
                return@callbackFlow
            }
        }

        val persona = personaDao.getPersona(personaId)
        val personaName = persona?.name ?: "AI"
        val personaDesc = persona?.description ?: "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"
        val memories = memoryDao.getRecentMemories(userId, personaId, 5)

        val systemInstruction = """
            Instructions:
            You are playing the role of "$personaName".
            Your character description: $personaDesc
            
            Rules:
            1. Act as $personaName at all times.
            2. DO NOT introduce yourself or repeat your description unless asked.
            3. Answer the user's latest question directly and naturally.
        """.trimIndent()

        val memoryContext = if (memories.isNotEmpty()) {
            "\nRelated Memories:\n" + memories.joinToString("\n") { "- ${it.content}" }
        } else ""

        val fullPrompt = buildString {
            append("<start_of_turn>user\n")
            append(systemInstruction)
            append("\n")
            append(memoryContext)
            append("\n\nUser's latest message: $userContent\n")
            append("<end_of_turn>\n")
            append("<start_of_turn>model\n")
        }

        try {
            llmInference?.generateResponseAsync(fullPrompt) { partialText, done ->
                if (!partialText.isNullOrEmpty()) {
                    trySend(partialText)
                }
                if (done) {
                    close()
                }
            }
        } catch (e: Exception) {
            trySend("Error: ${e.message}")
            close()
        }

        awaitClose { }
    }.flowOn(Dispatchers.IO)

    // æ€»ç»“è®°å¿†
    /**
     * åŠŸèƒ½: å¯¹å¯¹è¯è¿›è¡Œå‘¨æœŸæ€§æ‘˜è¦å¹¶æŒä¹…åŒ–åˆ°æœ¬åœ°è®°å¿†åº“ï¼Œä»¥å®ç°â€œè®°å¿†å…±ç”Ÿâ€ã€‚
     * å®ç°é€»è¾‘:
     * 1. é¢‘ç‡é˜ˆå€¼ï¼ˆæ¯5æ¬¡ï¼‰æ§åˆ¶ï¼Œé™ä½å†™å…¥ä¸æˆæœ¬
     * 2. æ„é€ åˆ†æ Promptï¼Œç”Ÿæˆäº‹å®æ€§æ‘˜è¦
     * 3. è¿‡æ»¤æ— æ•ˆç»“æœå¹¶å†™å…¥ Room
     * @param userId Long - ç”¨æˆ·ID
     * @param personaId Long - Persona ID
     * @param chatContent String - å¯¹è¯æ‹¼æ¥æ–‡æœ¬
     * @return Unit
     * å…³è”åŠŸèƒ½: REQ-C4 ç«¯äº‘ååŒ-è®°å¿†å½±å“ï¼›REQ-B5/B6 å…±ç”Ÿä¸è¡Œä¸ºå½±å“
     */
    suspend fun summarizeAndSaveMemory(userId: Long, personaId: Long, chatContent: String) = withContext(Dispatchers.IO) {
        val key = "${userId}_${personaId}"
        val count = messageCounters.getOrDefault(key, 0) + 1
        messageCounters[key] = count

        // é˜ˆå€¼æ£€æŸ¥
        if (count % 5 != 0) {
            Log.d("LocalLLM", "Skip memory gen. Count: $count (Threshold: 5)")
            return@withContext
        }

        if (llmInference == null) return@withContext

        Log.d("LocalLLM", "ğŸ”„ Starting memory summarization...")

        // [Fix] ä¿®æ”¹ Promptï¼šè®© AI è¿”å› "None" è€Œä¸æ˜¯ "æ— "ï¼Œé˜²æ­¢è¯¯ä¼¤åŒ…å« "æ— " çš„ä¸­æ–‡å¥å­ï¼ˆå¦‚ "æ— è¾£ä¸æ¬¢"ï¼‰
        val prompt = """<start_of_turn>user
Analysis Task:
Analyze the conversation below and extract specific details about the User (preferences, habits, plans, etc.).
- Output ONLY the extracted fact in a single sentence.
- If no useful information is found, output exactly "None".

Conversation:
$chatContent<end_of_turn>
<start_of_turn>model
"""
        try {
            val result = llmInference?.generateResponse(prompt)?.trim() ?: ""

            // [Debug] æ‰“å°åŸå§‹ç»“æœï¼Œæ–¹ä¾¿è°ƒè¯•
            Log.d("LocalLLM", "ğŸ“ Raw Summary Result: '$result'")

            // [Fix] ä¼˜åŒ–è¿‡æ»¤é€»è¾‘ï¼š
            // 1. ä¸ä¸ºç©º
            // 2. ä¸æ˜¯ "None" (å¿½ç•¥å¤§å°å†™)
            // 3. é•¿åº¦é€‚ä¸­ (æ”¾å®½åˆ° 100 å­—ç¬¦)
            val isValid =true

            if (isValid) {
                memoryDao.insertMemory(UserMemoryEntity(userId = userId, personaId = personaId, content = result))
                Log.d("LocalLLM", "ğŸ§  âœ… Memory Saved: $result")
            } else {
                Log.d("LocalLLM", "ğŸ—‘ï¸ Memory Discarded (Invalid/None/Too Long)")
            }
        } catch (e: Exception) {
            Log.e("LocalLLM", "âŒ Memory Gen Failed: ${e.message}")
            e.printStackTrace()
        }
    }
}
